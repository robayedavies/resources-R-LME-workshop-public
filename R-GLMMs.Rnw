
\documentclass{beamer}

\mode<presentation>
{
  \usetheme{CambridgeUS}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}


\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.
\usepackage{amsmath}

\graphicspath{{/Users/robdavies/Dropbox/resourcesimages/}}

\title
{Generalized Linear Mixed-effects models}

\author
{Rob Davies}

\institute
{r.davies1@lancaster.ac.uk}

\date
{Spring 2016}

\subject{Statistics}

\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{frame}
  \titlepage
\end{frame}

%\begin{frame}[allowframebreaks]
%  \frametitle<presentation>{Outline}
%  \tableofcontents
%\end{frame}



\section{Introduction}



\subsection{Aims for the class}



\begin{frame}{Aims for the class}
  \begin{enumerate}
  \item
  Understand the reasons for using Generalized Linear Mixed-effects models (GLMMs) to analyze discrete outcome variables
  \item
  Recognize the limitations of alternative methods for analyzing discrete outcome variables
  \item
  Practise running GLMMs with varying fixed or random effects structures
  \item
  Practise reporting the results of GLMMs
  \end{enumerate}  
\end{frame}



\section{Introduction to Generalized Linear Mixed-effects models}



\begin{frame}{Discrete outcome variables are very common}{Understand the reasons for using Generalized Linear Mixed-effects models (GLMMs) to analyze discrete outcome variables}
  \begin{itemize}
  \item<1->
  We often need to analyze outcomes that are discrete or categorical
  \begin{itemize}
  \item<2->
  The accuracy of responses (correct vs. incorrect)
  \item<3->
  The membership of one group out of two groups (e.g. impaired vs. unimpaired participant, fixation to left vs. right visual field)
  \item<4->
  Also, outcomes like: membership of one group out of multiple groups (categories); frequency of occurrence of an event; membership of ordered categories (e.g. Likert ratings scales)
  \end{itemize}
  \end{itemize}    
\end{frame}

\begin{frame}{Recognize the limitations of alternative methods for analyzing response accuracy}{You will see alternative methods used very frequently}
  \begin{itemize}
  \item<1->
  We often see response accuracy analyzed using one of the following methods:
  \begin{itemize}
  \item<2->
  The accuracy of responses (correct vs. incorrect) is counted e.g. as the number of correct responses (or errors) per subject, per condition
  \item<3->
  The raw number of correct or incorrect responses, or the percentage, or the proportion of responses that are correct or incorrect out of the total number of responses is analyzed as the outcome variable in ANOVA or regression
  \item<4->
  See the same approach if the outcome is group membership (e.g. impaired vs. healthy) or visual field (e.g. left vs. right) etc.
  \end{itemize}
  \end{itemize}    
\end{frame}

\begin{frame}{Recognize the limitations}{ANOVA or regression over proportions can lead to spurious results -- accuracy is bounded between 1 and 0, parametric model predictions or confidence intervals are not}
 \begin{columns}[T]
  \column{.4\textwidth}  
  \begin{itemize}
  	\item<1->
		 Each plot point shows percentage accuracy of responses per subject per learning trials block and experimental learning condition
		\item<2->	
		 Light grey lines show per-subject linear model best fit line for block effect
		\item<3->
		 Black lines show best fit for block effect for all participants
	\end{itemize}
   \column{.6\textwidth}
     \begin{figure}     
      \includegraphics[scale=0.195]{word-learning-lm-per-subject-for-report-010316}
      %\caption{Gavagai word learning study}
     \end{figure}  
\end{columns}     
\end{frame}

\begin{frame}{Recognize the limitations}{ANOVA or regression require the assumption of homogeneity of variance but for binary outcomes like accuracy the variance is proportional to the mean}
 \begin{columns}[T]
  \column{.4\textwidth}  
  \begin{itemize}
  	\item<1->
		 Given a binary outcome e.g. response is correct or incorrect
		\item<2->	
		 For every trial, probability $p$ that the response is correct
		\item<3->
		 The variance of the proportion of trials (per condition) with correct responses is dependent on $p$ and greater when $p\sim .5$
     %probability that a response will be correct
%     \item<4->
%      Variance will be greater where the probability of response accuracy is $\sim .5$
	\end{itemize}
   \column{.6\textwidth}
     \begin{figure}     
      \includegraphics[scale=0.3]{jaeger-2008-fig-1-variance-prop-mean}
      \caption{\footnotesize{Jaeger (2008) variance of sample proportion dependent on probability that a response is correct (or probability that is 1 of 0,1 outcomes)}}
     \end{figure}  
\end{columns}     
\end{frame}

% % jaeger 2008 p3 -- The expected sample proportion p over n trials is given by dividing \mu_X by the number of trials n, and hence is p (as \mu_X is np). Similarly, the variance of the sample proportion is a function of p: \sigma^2_p = \frac{p(1-p)}{n}
% % jaeger 2008 p3 -- In other words, if the probability of a binomially distributed outcome differs between two conditions, the variances will only be identical if p1 and p2 are equally distant from 0.5 (e.g. p1 = .4 and p2 = .6). The bigger the difference in distance from 0.5 between the conditions, the less similar the variances will be. Also, as can be see in Fig. 1, differences close to 0.5 will matter less than differences closer to 0 or 1. Even if p1 and p2 are unequally distant from 0.5, as long as they are close to 0.5, the variances of the sample proportions will be similar. Sample proportions between 0.3 and 0.7 are considered close enough to 0.5 to assume homogeneous variances (Agresti, 2002: 120). Within this interval, p(1􏰆p) ranges from 0.21 for p=.3 or .7 to .25 for p = .5. Unfortunately, we usually cannot determine a priori the range of sample proportions in our experiment (see also Dixon, this issue). Also, in general, variances in two binomially distributed conditions will not be homogeneous—contrary to the assumption of ANOVA.

\begin{frame}{Recognize the limitations of traditional methods for analyzing response accuracy}{The methods are common in the psychological literature but can give the wrong results}
  \begin{itemize}
  \item<1->
  Linear models assume outcomes are unbounded so allow predictions that are impossible when outcomes are, in fact, bounded as is the case for accuracy or other categorical variables
  \item<2->
  Linear models assume homogeneity of variance but that is unlikely and anyway cannot be predicted in advance when outcomes are categorical variables
  \item<3->
  If we are interested in the effect of an interaction, using ANOVA or linear models on accuracy (proportions of responses correct) can tell you, wrongly, that the interaction is significant
  \item<4->
  Common remedies e.g. the arcsine root transformation do not work (Jaeger, 2008)
  \end{itemize}    
\end{frame}

\begin{frame}{What we need, then, is a method that allows us to analyze categorical outcomes}{We find the appropriate method in Generalized Linear Models -- Generalized Linear Mixed-effects Models for repeated measures data}
     \begin{figure}     
      \includegraphics[scale=0.4]{jaeger-2008-fig-2-logit-probability-space}
      \caption{Jaeger (2008) the effect of some predictor x on a categorical outcome y: on the left the effect in logit space; on the right the effect in probability space}
     \end{figure}     
\end{frame}

\begin{frame}{What we need, then, is a method that allows us to analyze categorical outcomes}{The logistic transformation takes $p$ the probability of an event with two possible outcomes, and turns it into a logit: the natural logarithm of the odds of the event}
% see baguley pp 673-674 -- the transformation turns a discrete binary outcome into a continuous unbounded outcome
% -- also advantageous is the fact that odds and probabilities are both directly interpretable
% -- note that predictors have an additive relationship with respect to the log odds
% -- baguley notes that a final insight is that the logistic transformation maps differences in the predictors onto a nonlinear function with a particular form, the sigmoid curve
% -- to see the relationship between probability and predictors, can plot the inverse of the logistic function -- ie its cumulative distribution function p = \frac{e^x}{1 + e^x} -- the curve has an almost linear section in the middle where a normal linear model might provide a good fit but curves sharply at the extremes, reflecting the required non-linearity of effects at the boundary
% -- the curve produced by the inverse of the logistic function -- the function itself can be shifted up or down the x axis by adding a constant to the equation ie to the x eg the intercept in the systematic component of the linear model -- the curve can become steeper or shallower by multiplying the log odds by a constant eg changing the slope in a regression model -- changing the sign of the slope will change the direction of the slope so that the sigmoidal curve slopes down as it travels from left to right (required if the probability of success decreases as X increases)
  \begin{itemize}
  \item<1->
  The problem is how to estimate effects on a bounded outcome with a linear model
  \item<2->
  Transforming a probability to odds $o = \frac{p}{1-p}$ is a partial solution
  \item<3->
  Odds -- the ratio of the probability of occurrence to non-occurrence or of correct vs. incorrect -- are continuous and scaled from zero to infinity
  \item<4->
  Using the logarithm of the odds $logit = ln\frac{p}{1-p}$ removes the boundary at zero because log odds ranges from negative to positive infinity 
  % -- you can see this if you run the equation in R, replacing p with smaller and smaller numbers (e.g. $p = 0.1, 0.01, 0.001$) gets you increasing negative log odds
  \end{itemize}    
\end{frame}

\begin{frame}[fragile]{We can think of logistic models as working like linear models with log-odds outcomes}
\begin{equation}
ln\frac{p}{1-p} = logitp = \beta_0 + \beta_1X_1 \dots
\end{equation}
%  \begin{columns}[T]
%   \column{.6\textwidth}  
  \begin{itemize}
    \item<1->
% 		 Converting proabilities to odds means that estimated effects relate to outcome values ranging from 0 to positive infinity
%      % (odds of 1 $\sim$ proportion of .5)
% 		\item<2->	
		 We can describe the predicted log odds of a response of one type as the sum of the effects 
		\item<2->
     log odds range from negative to positive infinity (logit of 0 corresponds to proportion of .5)
	\end{itemize}
\end{frame}

\begin{frame}{To illustrate GLMMs we use the word learning data set}{Padraic Monaghan, our colleagues, and I examined the accuracy of responses in a word learning study: noun-verb-learning-study.csv}
% In our study, we directly compared learning of noun-object pairings, verb- motion pairings, and learning of both noun and verb pairings simultaneously, using an identical cross-situational learning task and environment in each case. We predicted that, consistent with previous studies of noun learning, word-object mappings would be learned from cross-situational statistics even when the utterances were complex and the observed dynamic scenes more complex than stationary figures. We also predicted that it would be possible to learn verbs from the same cross-situational statistics even though the names for the objects remained unknown, and that when nouns and verbs could be learned from the same utterance then cross-situational statistics would be sufficiently powerful to demonstrate learning for both these grammatical categories.
%      \begin{figure}     
%       \includegraphics[scale=0.35]{gavagai-accuracy-histogram}
%       \caption{Histogram showing frequency of correct vs. incorrect responses per subject per learning trials block and experimental learning condition}
%      \end{figure}     
\end{frame}

\begin{frame}{The word learning data set}
 \begin{columns}[T]
  \column{.6\textwidth}  
  \begin{itemize}
    \item<1->
    48 adults participated in learning trials (12 blocks of 24)
    \item<2->
    In each trial, observed 2 objects undergoing a different motion (one on the left, one on the right), and heard a sentence of fake words
    \item<3->
    Words were either assigned to ``refer to'' the objects (nouns) or to the motions (verbs)
    \item<4->
    Task is to indicate whether the heard sentence referred to the action on the left or the right of the screen to test if they could learn the object-or-motion to word associations
  \end{itemize}
   \column{.4\textwidth}
     \begin{figure}     
      \includegraphics[scale=0.2]{noun-verb-learning-study-methods-stimulus}
      \caption{\footnotesize{Example of a learning trial. Two moving objects are observed. Arrows indicate the movement path of the object. The four word phrase is simultaneously heard, with ``tha'' and ``noo'' function words and ``makkot'' and or ``pakrid'' referring to the motion and or object}}
      %, according to condition, in one of the scenes}}
      % The function words provided distributional information in the task such that they indicated the role of the word they preceded. Thus, in the noun-only condition, one of the function words always preceded the object referring word and the other function word preceded the non-referring words. In the verb-only condition, one of the function words preceded the motion-referring word and the other preceded the non-referring word. In the noun-and-verb condition, one of the function words preceded the object referring word and the other function word preceded the motion referring word. We selected function words to precede both nouns and verbs in the speech in order to control the distributional information for nouns and verbs. 
     \end{figure} 
     \end{columns}
\end{frame}

\begin{frame}{Phenomena and data sets in the social sciences often have a multilevel structure}{This is true for the word learning data set, which has a repeated measures design, requiring the use of mixed-effects models}
%      \begin{figure}     
%       \includegraphics[scale=0.4]{LME-multistage-sample-snijders-fig2-1}
%       \caption{Snijders \& Bosker (2012) Multistage sample}
%      \end{figure}     
\end{frame}

\begin{frame}{Variation in learning between participants}{We can reasonably attempt to model random effects of subjects on intercepts and the slope of the learning block and experimental condition effects}
     \begin{figure}     
      \includegraphics[scale=0.185]{all-merged-2-EDA-sort-by-percent-predicted-prob-and-condition-060214-2}
      \caption{\footnotesize{Graph showing the Generalized Linear Mixed-effects Model predicted values of the probability that a response is correct, for each participant, in each trial}}
     \end{figure}     
\end{frame}

\begin{frame}{A small change in R \emph{lmer} code allows us to extend what we know about linear mixed-effects models to conduct \emph{generalized linear mixed-effects models}}
\end{frame}

\begin{frame}[fragile]{Models varying in fixed effects with constant random effects (of subjects or items on intercepts)}{We start with an empty model}
\footnotesize{\begin{verbatim}
all.merged.glmm0 <- glmer(accuracy ~ 
                      
    (1|Subjecta) + (1|targetobject) + (1|targetaction),    
                          
    data = all.merged, family = binomial)

summary(all.merged.glmm0)
\end{verbatim}}
{}
\begin{itemize}
  \item<1->
	\verb:glmer(): the function name changes because now we want a \emph{generalized} linear mixed-effects model of accuracy
	\item<2->
  \verb:family = binomial: accuracy is a binary outcome variable so assume a binomial probability distribution
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Add a predictor variable coding for the effect of experimental condition}
\footnotesize{\begin{verbatim}
all.merged.glmm1 <- glmer(accuracy ~ 
                            
  Experiment +
                            
  (1|Subjecta) + (1|targetobject) + (1|targetaction),    
                          
   data = all.merged, family = binomial)
summary(all.merged.glmm1)
\end{verbatim}}
{}
\begin{itemize}
  \item<1->
	\verb:Experiment: learning conditions coded with the ``Experiment'' variable, a factor with levels ``nounonly'', ``verbonly'', ``nounverb''
  \item<2->
  Notice have included random effects of stimulus object and motion sample units (object items, motion-action items) on intercepts
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Add a predictor variable coding for learning trial block}
% {I think it is sensible to add both main effects in one step}
\footnotesize{\begin{verbatim}
all.merged.glmm2 <- glmer(accuracy ~ 
                            
    Experiment + block +
                            
    (1|Subjecta) + (1|targetobject) + (1|targetaction),    
                          
    data = all.merged, family = binomial)
summary(all.merged.glmm2)
\end{verbatim}}
{}
\begin{itemize}
  \item<1->
	\verb:block: there were 12 blocks of 24 learning trials, and here block is treated as a numeric variable
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Add effect of experimental condition and block interaction}
\footnotesize{\begin{verbatim}
all.merged.glmm3 <- glmer(accuracy ~ 
                            
    Experiment*block +
                            
    (1|Subjecta) + (1|targetobject) + (1|targetaction),    
                          
    data = all.merged, family = binomial)
summary(all.merged.glmm3)
\end{verbatim}}
\begin{itemize}
  \item<1->
	Notice that the \verb:(something)*(something): get you interactions and main effects for all possible pairs of variables
\end{itemize}
\end{frame}

\begin{frame}[fragile]{How do we know if increasing \emph{model complexity} by adding predictors actually helps us to account for variation in outcome values?}{The Likelihood Ratio Test (LRT) comparison tells us main and interaction effects are justified by improved model fit to data}
\tiny{\begin{verbatim}
anova(all.merged.glmm0, all.merged.glmm1, all.merged.glmm2, all.merged.glmm3)
\end{verbatim}}
{}
     \begin{figure}     
      \includegraphics[scale=0.385]{noun-verb-learning-study-anova-fixed}
      \caption{LRT comparisons of models varying in fixed effects}
     \end{figure}    
\end{frame}

\begin{frame}[fragile]{When the goal of a \alert{confirmatory} analysis is to test hypotheses about one or more critical fixed effects, what random-effects structure should one use?}
% {Examine the random effects}
\begin{itemize}
	\item<1->
	Current recommendations (Barr et al., 2013; JML): \alert{Maximal random effects structure}
	\item<2->
	If you are testing effects manipulated according to a prespecified -- confirmatory study -- design
	\begin{itemize}
	\item<3->
	Test random intercepts -- subjects and items
	\item<4->
	Test random slopes for all within-subjects or within-items fixed effects	
	\end{itemize}
  \item<5->
  But some authors (Bates et al., 2015; arXiv) argue we should test for the utility of adding model complexity
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Examine the utility of random effects by comparing models with the same fixed effects but varying random effects}{I am just going to assume we need both random effects of subjects and of items on intercepts so I focus on \emph{random slopes}}
\footnotesize{\begin{verbatim}
all.merged.glmm4 <- glmer(accuracy ~ 
                            
    Experiment*block +
                            
    (block + 1|Subjecta) + (block + 1|targetobject) 
    + (block + 1|targetaction),    
                          
    data = all.merged, family = binomial)
summary(all.merged.glmm4)
\end{verbatim}}
\begin{itemize}
	\item<1->
	\verb:(block + 1|Subjecta) ...: learning block is manipulated within-subjects and within-items -- so we must account for random variation between subjects, between stimulus objects, or between stimulus actions, in the block effect on response accuracy
\end{itemize}
\end{frame}

\begin{frame}[fragile]{The model summary indicates correlations between random intercepts and slopes for the items of 1}{Bates et al. (2015) argue this shows that model complexity cannot really be sustained -- we do not really need random effects of items on the slope of the block effect}
     \begin{figure}     
      \includegraphics[scale=0.275]{noun-verb-learning-study-glmer-random-slopes}
      % \caption{Model summary for model including random intercepts of subjects and item objects or actions on intercepts and slopes of the block effect}
     \end{figure} 
\end{frame}

\begin{frame}[fragile]{Extreme correlations (near 0 or 1) between random effects on intercepts and on slopes of fixed effects suggest the level of complexity in the model cannot really be justified}{Note also that the variances for the random effects of items on the slopes of the block effect are very small}
\begin{itemize}
  \item<1->
% 	Correlations between the random effects of items on intercepts and the slopes of the block effect suggest that the level
% of complexity we specify in this model cannot really be justified
%   \item<2->
%   Note also that the variances for the random effects of items on the slopes of the block effect are very small
%   \item<3->
  We should see if a simpler model, excluding the correlations between item random effectscan be estimated
\end{itemize}
     \begin{figure}     
      \includegraphics[scale=0.3]{noun-verb-learning-study-glmer-random-slopes}
      \caption{Model summary for model including random intercepts of subjects and item objects or actions on intercepts and slopes of the block effect}
     \end{figure} 
\end{frame}

\begin{frame}[fragile]{Try running the model removing just the problematic correlations -- item-wise -- between random effects}
\footnotesize{\begin{verbatim}
all.merged.glmm5 <- glmer(accuracy ~ 
                            
    Experiment*block +
                            
    (block + 1|Subjecta) + (1|targetobject) + (1|targetaction) + 
    (block + 0|targetobject) + (block + 0|targetaction),    
                          
     data = all.merged, family = binomial)
summary(all.merged.glmm5)
\end{verbatim}}
\begin{itemize}
  \item
	\verb:(1|targetobject) + (1|targetaction) + (block + 0|targetobject) + (block + 0|targetaction): the terms like \verb:(1|targetobject): specify a random effect (of items) on intercepts, while the terms \verb:(block + 0|targetobject): specify a random effect of items on the slope of the fixed effect (here, of block)
\end{itemize}
\end{frame}

\begin{frame}[fragile]{We want to simplify the model so we want to see no difference between the simpler and the more complex models}
\begin{itemize}
\item<1->
Compare models with random intercepts and slopes, and either (1.) correlations between all random intercepts and slopes (glmm4) or (2.) random effects and just correlations between intercepts and slopes for subjects random effects not for items random effects (glmm5)
\item<2->
If no difference between these models in relative fit then the item random effects correlations do not add anything to model utility 
% \item<3->
% Here we see that the simpler and more complex model are not different in how well they fit the data
\end{itemize}
     \begin{figure}     
      \includegraphics[scale=0.25]{noun-verb-learning-study-glmer-anova-corr-random-slopes}
%       \caption{Likelihood ratio test comparison: for models including random intercepts of subjects and item objects or actions on intercepts and slopes of the block effect -- with vs. without correlations between items random effects on intercepts or slopes}
     \end{figure} 
\end{frame}

\begin{frame}[fragile]{Reporting standards}{Model comparisons -- Using AIC, BIC and LRT}
% burnham & anderson 2001/p.3
\begin{itemize}
  \item<1->
	Report briefly the model comparisons e.g. ``Compared a simpler model: model 1, just main effects; model 2, main effects plus interactions''
	\item<2->
	Report the AIC or BIC for the different models, or LRT for pair-wise comparisons
	\begin{itemize}
	\item<3->
	Report and explain the model selection choice, based on the aims of the study and the information criteria comparisons results 
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Reporting the model}
\begin{itemize}
\item<1->
Summary of fixed effects -- just like in linear models -- with confidence intervals and p-values
\item<2->
Report random effects variance and covariance (if applicable)
%\item<3->
%In text, report likelihood comparisons
\end{itemize} 
    \begin{figure}     
     \includegraphics[scale=0.35]{R-GLMM-word-learning-project-summary}
     \caption{Model summary table -- word learning study -- GLMM}
    \end{figure}    
\end{frame}




\end{document}