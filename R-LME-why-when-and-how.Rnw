


\documentclass{beamer}

\mode<presentation>
{
  \usetheme{CambridgeUS}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}


\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.
\usepackage{amsmath}

\graphicspath{{/Users/robdavies/Dropbox/resourcesimages/}}
% enter in the {{}} whatever the file path will be to get to the images


\title
{Using Linear mixed-effects models -- why, when and how}

\author
{Rob Davies}

\institute
{r.davies1@lancaster.ac.uk}

\date
{May 2016}

\subject{Statistics}

\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{frame}
  \titlepage
\end{frame}



\section{Introduction}



\subsection{Aims for the class}


\begin{frame}{Aims for the class}
  \begin{enumerate}
  \item
  Understand the motivation for \emph{linear mixed-effects models} -- the requirements of handling multilevel structured data
  \item
  Introduce a multilevel structured dataset
  \item
  Recognize alternative methods for analyzing multilevel structured data
  \item
  Practise running linear mixed-effects models in R
  \item
  Evaluating models using information criteria
  \end{enumerate}  
\end{frame}



\section{Introduction to Linear mixed-effects models}



\subsection{Main ideas}


% -- see: Snijders, T.A.B., & Bosker, R.J. (1999). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage.

\begin{frame}{Phenomena and data sets in the social sciences often have a multilevel structure}{\emph{Repeated measures} or \emph{clustered data}}
\begin{itemize}
\item<1->
Test the same people multiple times
\begin{itemize}
\item<2->
Pre and post treatment
\item<3->
Multiple stimuli -- everyone sees the same stimuli
\item<4->
Repeated testing -- follow learning, development within individuals -- in longitudinal designs
\end{itemize}
\item<5->
Do multi-stage sampling
\begin{itemize}
\item<6->
Find (sample) classes or schools -- test (sample) children within classes or schools
\item<7->
Find (sample) clinics -- test (sample) patients within clinics
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The key insight: observations are clustered -- correlated -- \emph{not independent}}{Dependence of observations could be treated as a nuisance because an assumption of linear models is that observations are independent so failing to take dependence into account may result in incorrect inferences -- the non-independence of observations means you have less information than their total number of suggests you have}
\end{frame}

\begin{frame}{Where we are going: \emph{linear mixed-effects modelling}}{Capture sources of variance due to \emph{fixed effects} e.g. frequency and \emph{random effects} e.g. differences between sampling units like people or words in intercepts or slopes}
     \begin{figure}     
      \includegraphics[scale=0.2]{indiv-diffs-reading-perlearner-logrtxlogcelex-250210}
      \caption{Effect of word frequency on word naming latencies of adult students}
     \end{figure}     
\end{frame}


\subsection{ML study -- A concrete usage example}


\begin{frame}{In psychological research, uniformity - the average participant - is a convenient simplification}{We often average over individual differences to investigate experimental effects -- or we study differences between participant groups averaging over responses to different stimuli}
    \begin{figure}
		\includegraphics[scale=0.25]{crowd-korean-CC-Eric-Lafforgue}
		\end{figure}
\end{frame}

\begin{frame}{Both approaches cause problems but neither are necessary with linear mixed-effects models}{Given variability among individuals or stimuli, focusing on the average appears risky}
		\begin{figure}
		\includegraphics[scale=0.25]{crowd-CC-CatWalker}
		\end{figure}
\end{frame}

\begin{frame}{ML study -- A concrete usage example}{We can investigate systematic variation in effects by looking for \emph{interactions}}
	\begin{itemize}
	\item<1->
	Person-level effects: how reader attributes affect performance
	\item<2->
	Stimulus effects: how stimulus attributes affect performance
	\item<3->
	\emph{Interactions}: how stimulus effects are modulated by person-level effects
	\end{itemize}
\end{frame}

\begin{frame}{ML study -- A concrete usage example}{The data-set -- experimental reading task -- lexical decision}
\begin{itemize}
		\item
		All participants saw all 160 words and 160 matched non-words
		\item
		Effects of \emph{TOWRE} measures of reading skill, age, ART measure of print exposure
		\item		
		Effects of word attributes like item type (e.g. responses to words vs. non-words)
		\item
		Interactions between effects of \emph{who} you are and effects of \emph{what} kinds of stimuli you must respond to
\end{itemize}
\begin{figure}
		\includegraphics[scale=0.25]{subjects-behaviour-items-310114-csv}
		\end{figure}
\end{frame}

\begin{frame}[fragile]{Get the data for practice -- download and read in the ML datset of responses to words and nonwords}{Having read in \emph{subjects.behaviour.items-310114.csv}, use \emph{subset()} to remove errors}
\small{\begin{verbatim}
ML.all <- read.csv("subjects.behaviour.items-310114.csv", 
header=T, na.strings = "-999")

ML.all.correct <- subset(ML.all, RT > 200)
summary(ML.all.correct)
\end{verbatim}}
\begin{figure}
  	\includegraphics[scale=0.25]{summary-ML-all-correct}
		\end{figure}
\end{frame}



\section{From linear models to linear mixed-effects models}



\subsection{Dealing with repeated-measures data}


\begin{frame}[fragile]{A linear model (multiple regression) of an effect of some set of conditions}{Linear models assume independence of observations -- but if ou take repeated measures then observations will be \emph{dependent} -- responses will cluster by person or stimulus}
	\begin{equation}
		X_{ij} = \mu + (\mu_j - \mu) + \varepsilon_{ij} = \mu + \tau_j + \varepsilon_{ij}
	\end{equation}
\begin{itemize}	
	\item<1->
	$X_{ij}$ -- the score of person $i$ in condition $j$
	\item<1->
	$\mu$ -- the mean of all subjects who could be tested in the experiment
	\item<1->
	$\mu_j$ -- the mean score in condition $j$
	\item<2->
	$\tau_j$ -- the extent to which the mean for condition $j$ is different from the overall mean
	\item<1->
	$\varepsilon_{ij}$ -- the amount to which person $i$ in condition $j$ differs from the mean for that group
\end{itemize}
\end{frame}

% \begin{frame}[fragile]{If you take repeated measures then observations will be \emph{dependent} -- correlated -- within each person}{For a slow responder, all their responses will be slow}
% \begin{itemize}	
% 	\item<1->
% 	Linear models assume independence of observations
% 	\item<2->
% 	One way to take the dependence out is by centring all observations for each person on the means (for each person)
% %	\item<3->
% %	Faust et al. (1999) recommend standardising scores before modelling -- for each score, remove the subject mean and divide by the subject SD of scores
% %	\item<4->
% %	If we remove subject means then we take out -- effectively we \emph{partial out the differences between subjects}
% %	\item<5->
% %	We can then isolate the treatment effect we care about	
% \end{itemize}
% \end{frame}

\begin{frame}[fragile]{We can account for between and within subject differences in our model}{If you take repeated measures then observations will be \emph{dependent}: for a slow responder, all their responses will be slower; for a difficult stimulus, all responses will be slower}
	\begin{equation}
		X_{ij} = \mu + \alert{\pi_i} + \tau_j + \varepsilon_{ij}
	\end{equation}
\begin{itemize}	
	\item<1->
	$X_{ij}$ -- the score of person $i$ in condition $j$
	% -- grand mean
	\item<1->
	$\mu$ -- the mean of all subjects who could be tested in the experiment
	\item<2->
	\alert{$\pi_i$ -- add effect of being subject $i$ -- compared to average over all subjects}
	% subjects (grand mean)}
	\item<1->
	$\tau_j$ -- add effect of being in condition $j$ -- compared to average over all conditions 
	%(grand mean)
	\item<1->
	$\varepsilon_{ij}$ -- the amount to which person $i$ in condition $j$ differs from the mean for that group
\end{itemize}
\end{frame}

\begin{frame}[fragile]{A more realistic repeated measures model}{Suppose that effects vary between subjects}
	\begin{equation}
		X_{ij} = \mu + \tau_j + \alert{\pi_i + \pi_i\tau_j} + \varepsilon_{ij}
	\end{equation}
\begin{itemize}	
	\item
	$X_{ij}$ -- the score of person $i$ in condition $j$ -- grand mean
	\item
	$\mu$ -- the mean of all subjects who could be tested in the experiment
  \item
	$\tau_j$ -- add effect of being in condition $j$ -- compare average over all conditions (grand mean)
  \item
	\alert{$\pi_i$ -- add effect of being subject $i$ -- compare average over all subjects (grand mean)}
	\item
	\alert{$\pi_i\tau_j $ -- a subject by treatment interaction -- different subjects (or words) react to conditions in different ways}
	\item
	$\varepsilon_{ij}$ -- the amount to which person $i$ in condition $j$ differs from the mean for that group
\end{itemize}
\end{frame}


\subsection{The language as fixed effect fallacy}


\begin{frame}{\emph{The language as fixed effect fallacy} \tiny{Clark, 1973}}
%{We need to deal with effects of random variation due to random differences between stimuli as well as differences between people}
\begin{itemize}  
	\item<1->
	Historically, psychologists estimated effects taking into account error variance due to random differences between participants' responses
	\item<2->
	But ignoring stimulus sampling could mean that effects are inferred to exist even when differences between conditions are due to random differences between responses to different stimuli
\end{itemize}
\end{frame}

\begin{frame}[fragile]{A linear model taking into account \emph{the random effects of items}}
	\begin{equation}
		X_{ij} = \mu + \pi_i + \tau_j + \pi_i\tau_j + \alert{\beta_k +   \pi_i\beta_k}  + \varepsilon_{ijk}
	\end{equation}
\begin{itemize}	
	\item<1->
	 \alert{$\beta_k$ -- effect of word $k$} -- unexplained differences in average response elicited by different stimuli
	\item<2->
	 \alert{$\pi_i\beta_k$ -- the stimulus word by subject interaction} -- different people respond to different stimuli differently
\end{itemize}
\end{frame}

\begin{frame}{Taking into account error variance due to subjects and items}{Clark's (1973) $minF'$ solution}
\begin{equation}
			minF' = \frac{MS_{\tau}}{MS_{\pi\tau} + MS_{\beta_k}} = \frac{F_1F_2}{F_1 + F_2}
\end{equation}
\begin{enumerate}
	\item<1->
	You start by \emph{aggregating} your data
	\item<2->
	By-subjects data -- for each subject, take the average of their responses to all the items
	\item<3->
	By-items data -- for each item, take the average of all subjects' responses
	\item<4->
	You do separate ANOVAs, one for by-subjects (F1) data and one for by-items (F2) data
	\item<5->
	 You put F1 and F2 together in the calculation of minF' \emph{though psychologists tended to stop at step 4}
\end{enumerate}
\end{frame}


\subsection{Repeated measures regression analysis}


\begin{frame}{The problem with minF' is that it is only good for ANOVA and ANOVA is only good for testing the effects of categorical variables -- \emph{factors}}{Many dealt with the Clark problem, and allowed themselves to include predictors that were continuous variables, by performing regression analyses of by-items data}
     \begin{figure}     
      \includegraphics[scale=0.265]{indiv-diffs-reading-perlearner-logrtxlogcelex-250210}
      \caption{Effect of word frequency on word naming latencies of adult students}
     \end{figure}     
\end{frame}

\begin{frame}{Repeated measures regression analysis}{The problem with regression on by-items means data}
\begin{itemize}
	\item<1->
	Lorch \& Myers (1990) argued there is a problem with multiple regression on by-items mean observations
	\item<2->
 	The approach reverses the language-as-fixed-effect problem
	\item<3->
	Effects are assessed by comparison with an item-based error term
	\item<4->
	Effects can be significant because of random variation between subjects in how they responded to items
\end{itemize}
\end{frame}

\begin{frame}{Repeated measures regression analysis}{The problem with regression on by-items means data}
\begin{itemize}
	\item<1->
	Lorch \& Myers (1990) suggested solutions to the problem with multiple regression on by-items mean observations
	\item<2->
	A common approach is to perform a regression (linear model) on each subject and complete a t-test or ANOVA on the resulting per-subject coefficients
  \item<3->
  Code for subject with $n-1$ dummy variables and complete regression using subject, and subject by effect, predictors
  \end{itemize}
\end{frame}

\begin{frame}{Analyses of results with simulated data and alternate procedures suggested that the per-subjects regression approach does not work well \emph{\tiny{Baayen et al. (2008)}}}{You can see how problems can arise -- the approach cannot take into account variation in the reliability of estimates}
\begin{figure}     
     \includegraphics[scale=0.35]{why-how-when-per-subj-estimates}
      \caption{Per-subject estimates (points) with standard errors -- ML data-set -- intercepts and the effect of item type (words vs. non-words)}
     \end{figure}     
% \begin{figure}     
%      \includegraphics[scale=0.45]{LME-baayen-2008-data-simulation-analyses}
%       \caption{Baayen et al. (2008): simulated data with or without effects present: item = by-items means regression; lmerS = LM90 per-subject regression approach}
%      \end{figure}     
\end{frame}

% \begin{frame}{If you do repeated measures studies of any kind, you need to take the `language-as-fixed-effect fallacy' into account -- participant and stimulus random effects}
% \end{frame}


\subsection{Shift to mixed-effects models}


% \begin{frame}{Dealing with clustered data -- start by ignoring the multilevel structure}{You could try to run an ordinary linear model including subject-level and item-level variables}
% 	\begin{equation}
% 	RT = \beta_0 +  \beta_{word reading ability} + \beta_{itemtype} + \beta_{word*itemtype} + \epsilon
% 	\end{equation}
% \begin{itemization}
% \item<1->
% But it is usually incorrect to assume the multilevel structure can be represented by the explanatory variables alone
%   \begin{itemize}
% 	\item<2->
% 	What about effect of \emph{random} variation between \emph{participants}?
% 	\item<2->
% 	What about effect of \emph{random} variation between \emph{stimuli}?
% 	\end{itemize}
% \end{itemize}
% \end{frame}

\begin{frame}[fragile]{A more realistic repeated measures model of the item type and reading ability effects}{The model is better because it takes into account random effects of subjects and items -- we could do this by adding predictors that coded for participant or item identity, and for the effects of interactions between effect of participant and effect of stimulus type}
	\footnotesize{\begin{equation}
	RT = \beta_0 +  \beta_{word reading ability} + \beta_{itemtype} + \beta_{ability*itemtype} + 
	\emph{\beta_{subject}} + \beta_{item} + \emph{\beta_{subject*itemtype}} + \epsilon
	\end{equation}}
\begin{itemize}
	\item<1->
	 Allow \emph{intercept to vary} -- random effect of subject -- some have slower average some have faster average than average overall
	\item<3->
	 Allow \emph{slope of item type effect to vary} -- random effect of subject -- subjects can experience effects of (item type) with different directions or sizes
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{A more realistic repeated measures model of the item type and reading ability effects}{Taking into account random effects of subjects and items}
	\footnotesize{\begin{equation}
	RT = \beta_0 +  \beta_{word reading ability} + \beta_{itemtype} + \beta_{ability*itemtype} + 
	\beta_{subject} + \emph{\beta_{item}} + \beta_{subject*itemtype} + \epsilon
	\end{equation}}
\begin{itemize}
	\item<1->
	What about effect of random variation between stimuli?
		\begin{itemize}
		\item<2->
		Allow \emph{intercept to vary} -- random effect of items -- some items harder and elicit slower average response and some easier and elicit faster responses on average than average overall
		\item<3->
		Allow \emph{effect of reading ability to vary} -- within-items effect of subject type can be different for different items
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{We do not model random effects directly -- we just estimate the \emph{spread} or variance of random differences}{\emph{random intercepts} -- predicted differences (adjustments) between the overall average and the group e.g. person average -- and \emph{random slopes} -- predicted differences (adjustments) between the overall effect and the group e.g. per-person effect}
\begin{figure}     
      \includegraphics[scale=0.2]{RT-words-freq-persubj-loess}
      \caption{Individual differences in intercepts and in the effect of word frequency on lexical decision RTs}
     \end{figure}  
\end{frame}    

\begin{frame}{The advantages of mixed-effects models}
\begin{itemize}
	\item<1->
	Approach is not restrictive about predictors or data structure
	% -- compared to minF ANOVA
	\begin{itemize}
	\item<2->
	ANOVA is OK for experimental designs, categorical factors, data sets without missing values
	\end{itemize}
	\item<3->
	Can test effects at different levels of hierarchy
	\item<3->
	We can allow random effects of both subjects and items -- solving the `language-as-fixed-effect' problem
  \item<4->
  Estimation robust to imbalances in data
\end{itemize}
\end{frame}



\section{Running LME code in R}


\begin{frame}[fragile]{We focus on building a series of models up to the most complex model supported by the data}
\begin{itemize}
 \item<1->
	A minimal (empty) model of the data might assume that the data we observe can be predicted only given the average value of observations -- intercept 
%	\begin{itemize}
	\item<2->
	And random effects of grouping variables like subjects or stimulus items on the average outcome
\end{itemize}
% 	\item<3->
% 	The question is then whether our capacity to predict observations is improved by adding other terms
% \end{itemize}
\end{frame}

% \begin{frame}[fragile]{Examine the fixed effects then the random effects}
% \begin{itemize}
% 	\item<1->
% 	Start by examining models varying in the \emph{fixed effects} but constant in the \emph{random effects}
% 	\item<2->
% 	Fitted using maximum likelihood ($REML=FALSE$) method
% 	\begin{itemize}
% 	\item<3->
% 	Think about simpler models being \emph{nested} inside -- i.e. as simplifications of -- more complex models
% 	\end{itemize}
% 	\item<4->
% 	Add effects of interest -- because they were manipulated, are of theoretical or practical interest -- fixed effects in series	
% \end{itemize}
% \end{frame}

\begin{frame}[fragile]{R code for running a linear mixed-effects model}{Start with an empty model specifying just the fixed effect of the intercept (overall average outcome) and the random effects of subjects and of items on intercepts (random differences in average outcomes)}
\begin{verbatim}
full.lmer0 <- lmer(logrt ~
                    (1|subjectID) + (1|item_name),
 data = ML.all.correct)
\end{verbatim}
\begin{itemize}
	\item<1->
	\verb:lmer(): run a \emph{Linear Mixed-effects model} using the \emph{lmer()} function
	\item<2->
  \verb:full.lmer0 <- lmer(): the model creates an object \emph{full.lmer0} we can work with e.g. to get a summary of estimates
  \item<3->
  \verb:logrt ~ (1|subjectID) + (1|item_name): the \emph{lmer()} function interprets the model formula we specify
  \item<4->
  \verb:data = ML.all.correct: applying that model to the data we name \emph{ML.all.correct} 
\end{itemize}
\end{frame} 

\begin{frame}[fragile]{R code for running a linear mixed-effects model}{Start with an empty model specifying just the fixed effect of the intercept (overall average outcome) and the random effects of subjects and of items on intercepts (random differences in average outcomes)}
\begin{verbatim}
full.lmer0 <- lmer(logrt ~
                     (1|subjectID) + (1|item_name),
data = ML.all.correct)
\end{verbatim}
\begin{itemize}
  \item<1->
   \verb:(1|subjectID) + (1|item_name): specify random effects
	\begin{itemize}
	 \item<2->
	 \verb:(1|...): -- random effect on intercepts
	 \item<3->
	 \verb:subjectID:  or \verb:item_name: -- effects of subjects or items -- specified by subject identity code (ID) or item name in coding variables
  \end{itemize}
  \item<4->
  The intercept \verb:logrt ~ 1: is taken as required automatically, given the random intercepts terms
\end{itemize}
\end{frame}

\begin{frame}[fragile]{R code for running a linear mixed-effects model}{An empty model specifying just the fixed effect of the intercept and random effects of subjects and of items on intercepts}
\begin{verbatim}
full.lmer0 <- lmer(logrt ~
                     (1|subjectID) + (1|item_name),
data = ML.all.correct)
summary(full.lmer0)
\end{verbatim}
\verb:summary(full.lmer0): will deliver model estimates
\begin{figure}     
      \includegraphics[scale=0.2]{ML-all-correct-lmer-0-summary}
      \caption{Notice the random effects (variance) components and the fixed effect (intercept) summary}
     \end{figure}  
\end{frame}  

\begin{frame}[fragile]{R code for running a linear mixed-effects model -- \emph{notice the moving parts}}
\begin{equation}
\alert{full.lmer0} <- lmer(\alert{logrt} \sim (1|\alert{subjectID}) + (1|\alert{itemname}), data = \alert{ML.all.correct})
\end{equation}
\begin{itemize}
  \item<1->
	You specify the model name \alert{full.lmer0}, the dependent variable \alert{logrt}, the subject and item coding variables \alert{subjectID}, \alert{itemname}, and the name of the data frame \alert{ML.all.correct}
  \item<2->
  You need to have previously used \emph{read.csv()} to enter the data frame in R's workspace, and you need to know what the variables in the dataframe are called
\end{itemize}
\end{frame} 

% \begin{frame}[fragile]{REML and ML estimation and model comparison}
% \begin{verbatim}
% full.lmer0 <- lmer(logrt ~
%                     
%                     (1|subjectID) + (1|item_name),
%                   
% data = subjects.behaviour.items.nomissing, REML = F)
% \end{verbatim}
% \begin{itemize}
% 	\item<1->
% 	\verb:REML = F: -- maximum likelihood estimation
% 	\item<2->
% 	Maximum likelihood estimation seeks to find those parameter values that, given the data and our choice of model, make the model's predicted values most similar to the observed values
% \end{itemize}
% \end{frame}

\begin{frame}[fragile]{LMEs -- build-up the \emph{fixed} effects while holding the \emph{random} effects constant}
% {To empty model, for the ML study analysis, add subject attribute variables as predictors}
\footnotesize{\begin{verbatim}
full.lmer1 <- lmer(logrt ~
                  
zAge + zTOWRE_wordacc + zTOWRE_nonwordacc +
                     
(1|subjectID) + (1|item_name),
data = ML.all.correct, REML = F)
summary(full.lmer1)
\end{verbatim}}
\begin{itemize}
	\item<1->
	\verb:zAge + zTOWRE_wordacc: add fixed effects -- z- because they were standardized
	\item<2->
 	\emph{Fixed effects} reproducible effects -- manipulated, selected -- of theoretical or practical interest
	\item<3->
	\verb:summary(full.lmer1): -- print a model summary
\end{itemize}
\end{frame}

\begin{frame}[fragile]{LMEs -- build-up the \emph{fixed} effects while holding the \emph{random} effects constant}
% {To empty model, for the ML study analysis, add subject attribute variables as predictors}
\footnotesize{\begin{verbatim}
full.lmer1 <- lmer(logrt ~
zAge + zTOWRE_wordacc + zTOWRE_nonwordacc + (1|subjectID) + (1|item_name),
data = ML.all.correct, REML = F)
summary(full.lmer1)
\end{verbatim}}
\begin{figure}     
      \includegraphics[scale=0.2]{ML-all-correct-lmer-1-summary}
      \caption{Notice the fixed effect coefficients estimates are presented in a regression style table}
     \end{figure}
\end{frame}

\begin{frame}{How do we know if increasing \emph{model complexity} by adding predictors actually helps us to account for variation in outcome values?}{Simplicity and parsimony -- we look at evaluating models, next}
\begin{itemize}
 \item<1->
	Trade-off between too much and too little simplicity in model selection -- variable selection
	\begin{itemize}
	\item<2->
	Models with too many parameters may tend to identify effects that are spurious
	\item<3->
	Effects may be \emph{unintuitive} and hard to explain \emph{and} not reproduced in future samples
	\end{itemize}			
\end{itemize}
\end{frame}




\end{document}